---
layout: post
title:  "Methodology Answers"
date:  2017-03-10 08:57:00 +0100
categories: study
---

I outlined several questions in my [last post]({{ site.baseurl }}{% post_url 2017-01-12-methodology-questions %}) about the methodology I'm going to be using for my first PhD study. This post will hopefully answer those questions.

#### Formative Testing

I visited a local primary school to test an alpha version of the software that will be used for the pre-and post-test, and to see how children handled LightbotJr. Y1's were given *some* assistance by Y6's, although they were told limit this.

The alpha version of the software had:

- 9 questions (more detail on these below).
- No introduction.
- Was entirely drag and drop (no click and click functionality).

LightbotJr

- Children played this for about 10 minutes.
- Most reached a few levels in to the game and seemed suitably comfortable when playing it.

#### Ceiling Effect (In the pre-and post-test)

I want to reduce the probability that the ceiling effect will occur during the pre-and post-tests. To do this, I'm going to have a large number of questions and a time limit, so the participants are unlikely to complete all the questions. This should give a wider range of scores, and is what I've done in previous studies.

From the formative testing of my sequencing application (more on this later), Y1 children tend to take around 30 seconds to complete a question. If the time limit was 5 minutes, on average they'd complete 10 questions in this time. If my test is 15 questions, the participants are unlikely to reach the end of those, and if they do they will have rushed and probably made mistakes.

**In summary**

- 20 questions for both the pre-test and post-test.
- 5 minute time limit, that isn't intrusive (so no timer on screen) and allows them to complete the current question before ending the test.
- Using the scoring system of previous studies (2 for correct sequence, 1 for correct end to the sequence, 0 for incorrect sequence.)

#### Validity of Assessment

The pre-and post-test questions need to be a suitible difficulty for children aged 5 and 6. The 9 Y1 children used for the formative testing of the software scored 13.78/20 points on average, showing that the difficulty of the current questions is suitible (if not slightly easy). These 9 questions were taken from the following sources:

- Baron Cohen, et al (1986).
	- Taking bear when not looking (Itentional)
	- Kicking ball in to river (Mechanical 2)
- Kazakoff, et al (2013).
	- Log falling off waterfall (Mechanical 1)
	- Stealing toy train (Behavioural 1)
	- Taking ice cream (Behavioural 2)
- The all kids network (2014).
	- Building a snowman (Behavioural 2)
	- Planting flowers (Mechanical 2)
	- Going to bed (Behavioural 1)
	- Winning a race (Behavioural 1)

A quick reminder of what each category means:

- Mechanical 1 (objects interacting causally with each other)
- Mechanical 2 (people and objects acting causally on each other)
- Behavioural 1 (a single person acting out everyday routines)
- Behavioural 2 (people acting in social routines)
- Intentional (people acting in everyday activities requiring the attribution of mental states)

I also collected data on the questions of each type, including the score on each question and the time it took the participant to submit their answer (with outliers removed.)

- Mechanical 1, 1 question(s), 34.81 seconds, 72.22% correct
- Mechanical 2, 2 question(s), 43.72 seconds, 94.74%correct
- Behavioural 1, 2 question(s), 35.73 seconds, 83.33% correct
- Behavioural 2, 3 question(s), 32.64 seconds, 62.96% correct
- Intentional, 1 question(s), 34.44 seconds, 61.11% correct

As long as I keep an even split of the question types, then I should get a suitably wide range of scores from participants. Or they could be weighted by the difficulty above, as behavioral and intentional questions seem to be the most difficult. Also, **for the actual study I need to record time from the first movement of the mouse or have a start button because the time for the first questions were all outliers.**

I need 30 questions in total, so that would be 6 questions of each type. Time to get drawing!

#### Ability Level

The cognitive and motor ability of the target age group was a concern in the previous post. The formative testing did show that children have the ability to use both the sequencing assessment and Lightbot.

The second application the active control group will be used is one of the [Twinkl Phonics apps](http://www.twinkl.co.uk/page/twinkl-apps).

#### Groupings

It's now been confirmed that the study will take place in a primary school with 90 children per year group. The children will do the pre-test in groups of 15 or 30, depending on how comfortable the teacher is in doing this.

#### Intervention

The children will be split in to 2 groups, one playing Lightbot and one using the Phonics app. Groups randomised across classes to reduce the impact that a single teacher might be having on the dependant variable in the study.

The year group has access to 15 iPads that will be used for the intervention, and have agreed to install both LightbotJr and the phonics app.

#### Summary

So all in all, it's looking pretty good. I need to make some changes to the testing software, so that will be the next step and possibility the next blog post. The study is penciled in to take place at the start of May, so I have about 8 weeks to perfect it.

#### Sources

Baron-Cohen, S., A. M. Leslie, and U. Frith, ‘Mechanical, Behavioural and Intentional Understanding of Picture Stories in Autistic Children’, British Journal of Developmental Psychology, 4:2 (1986), pp. 113–125.

Kazakoff, E. R., A. Sullivan, and M. U. Bers, ‘The Effect of a Classroom-Based Intensive Robotics and Programming Workshop on Sequencing Ability in Early Childhood’, Early Childhood Education Journal, 41:4 (2013), pp. 245–255.

The All Kids Network, https://www.allkidsnetwork.com/sequencing/
